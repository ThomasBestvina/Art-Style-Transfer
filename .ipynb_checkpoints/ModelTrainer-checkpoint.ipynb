{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00cbff1b-0e68-407b-aced-b335f182e3db",
   "metadata": {},
   "source": [
    "# Initalize and install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f9cf912-9a35-438b-a72b-97420ef38c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4ae0fc3-a577-4f66-a29b-320cbcce30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"\"\n",
    "monet_path = \"inputs/\"\n",
    "photo_path = \"outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c1e344b-cf45-4012-9b35-966165b1ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonetPhoto(Dataset):\n",
    "    def __init__(self, data_root, monet_path, photo_path, transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.monet_path = monet_path\n",
    "        self.monet_images = os.listdir(os.path.join(data_root, monet_path))\n",
    "        self.photo_path = photo_path\n",
    "        self.photo_images = os.listdir(os.path.join(data_root, photo_path))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.monet_images), len(self.photo_images))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            monet_image = Image.open(os.path.join(data_root, monet_path, self.monet_images[idx % len(self.monet_images)]))\n",
    "            photo_image = Image.open(os.path.join(data_root, photo_path, self.photo_images[idx % len(self.photo_images)]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image at index {idx}: {e}\")\n",
    "            # Handle the error by skipping this sample, or logging it\n",
    "            raise Exception(f\"Error loading image at index {idx}: {e}\");\n",
    "        if self.transform:\n",
    "            monet_image = self.transform(monet_image)\n",
    "            photo_image = self.transform(photo_image)\n",
    "        return monet_image, photo_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ad9589-2bc3-4163-9af4-d43d1f95c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((256,256)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "dataset = MonetPhoto(data_root, monet_path, photo_path, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f59e99f7-ac7b-4660-a58c-aab0684bd30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, test_size = int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95eb5b4c-9421-44f4-854c-c2092645c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ced2d519-8979-4080-b0c4-b686d0dc5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data.dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "test_dataloader = DataLoader(testing_data.dataset, batch_size=32, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fa8bec3-5390-4dd2-b13a-da9318d7bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c930b3f3-71e3-497c-a3b7-5d0b9133cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=2, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, stride, bias=False, padding_mode='reflect', device=self.device),\n",
    "            nn.BatchNorm2d(out_channels, device=self.device),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7ea7936-75a6-4f72-b368-566c272bfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512], device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], kernel_size=4, stride=2, padding=1, padding_mode='reflect', device=self.device),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(\n",
    "                CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2, device=self.device),\n",
    "            )\n",
    "            in_channels = feature\n",
    "        \n",
    "        layers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode='reflect', device=self.device\n",
    "            )\n",
    "        )\n",
    "            \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d10fa954-1864-4373-b353-e12962e1c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, act = 'relu', use_dropout=False, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode='reflect', device=self.device)\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False, device=self.device),\n",
    "            nn.BatchNorm2d(out_channels, device=self.device),\n",
    "            nn.ReLU() if act=='relu' else nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.dropout(x) if self.use_dropout else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "148d9e22-2e31-4e60-a8d8-b22bc2ea281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=64, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.initial_down = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode='reflect', device=self.device),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        ) #128\n",
    "        \n",
    "        self.down1 = Block(features, features*2, down=True, act='leaky', use_dropout=False)   #64\n",
    "        self.down2 = Block(features*2, features*4, down=True, act='leaky', use_dropout=False) #32\n",
    "        self.down3 = Block(features*4, features*8, down=True, act='leaky', use_dropout=False) #16\n",
    "        self.down4 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False) #8\n",
    "        self.down5 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False) #4\n",
    "        self.down6 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False) #2\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*8, 4, 2, 1, padding_mode='reflect', device=self.device), #1x1\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = Block(features*8, features*8, down=False, act='relu', use_dropout=True)\n",
    "        self.up2 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=True)\n",
    "        self.up3 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=True)\n",
    "        self.up4 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=False)\n",
    "        self.up5 = Block(features*8*2, features*4, down=False, act='relu', use_dropout=False)\n",
    "        self.up6 = Block(features*4*2, features*2, down=False, act='relu', use_dropout=False)\n",
    "        self.up7 = Block(features*2*2, features, down=False, act='relu', use_dropout=False)\n",
    "        self.finil_up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*2, in_channels, kernel_size=4, stride=2, padding=1, device=self.device),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        d1 = self.initial_down(x)\n",
    "        d2 = self.down1(d1)\n",
    "        d3 = self.down2(d2)\n",
    "        d4 = self.down3(d3)\n",
    "        d5 = self.down4(d4)\n",
    "        d6 = self.down5(d5)\n",
    "        d7 = self.down6(d6)\n",
    "        \n",
    "        bottleneck = self.bottleneck(d7)\n",
    "        \n",
    "        up1 = self.up1(bottleneck)\n",
    "        up2 = self.up2(torch.cat([up1, d7], 1))\n",
    "        up3 = self.up3(torch.cat([up2, d6], 1))\n",
    "        up4 = self.up4(torch.cat([up3, d5], 1))\n",
    "        up5 = self.up5(torch.cat([up4, d4], 1))\n",
    "        up6 = self.up6(torch.cat([up5, d3], 1))\n",
    "        up7 = self.up7(torch.cat([up6, d2], 1))\n",
    "        \n",
    "        return self.finil_up(torch.cat([up7, d1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee68a93e-f778-4df9-ab72-95b8276f82de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                           \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 55.94 MiB is free. Including non-PyTorch memory, this process has 3.75 GiB memory in use. Of the allocated memory 3.60 GiB is allocated by PyTorch, and 90.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 237\u001b[0m\n\u001b[1;32m    235\u001b[0m trainer\u001b[38;5;241m.\u001b[39mload_model()\n\u001b[1;32m    236\u001b[0m trainer\u001b[38;5;241m.\u001b[39minit_optimizers()\n\u001b[0;32m--> 237\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 66\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Train step\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m g_loss, d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m train_g_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m g_loss\n\u001b[1;32m     69\u001b[0m train_d_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_loss\n",
      "Cell \u001b[0;32mIn[16], line 138\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    135\u001b[0m fake_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(x)\n\u001b[1;32m    137\u001b[0m )\n\u001b[0;32m--> 138\u001b[0m real \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    140\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(real, torch\u001b[38;5;241m.\u001b[39mones_like(real, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m    141\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(fake_, torch\u001b[38;5;241m.\u001b[39mzeros_like(fake_, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    144\u001b[0m d_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial(x)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mCNNBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/modules/activation.py:828\u001b[0m, in \u001b[0;36mLeakyReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaky_relu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_slope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/grproc/lib/python3.10/site-packages/torch/nn/functional.py:1902\u001b[0m, in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1900\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mleaky_relu_(\u001b[38;5;28minput\u001b[39m, negative_slope)\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaky_relu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_slope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 55.94 MiB is free. Including non-PyTorch memory, this process has 3.75 GiB memory in use. Of the allocated memory 3.60 GiB is allocated by PyTorch, and 90.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data: DataLoader,\n",
    "        val_data: DataLoader,\n",
    "        generator: torch.nn.Module,\n",
    "        discriminator: torch.nn.Module,\n",
    "        nb_epochs: int = 5,\n",
    "        device: str = \"cuda\",\n",
    "        save_path: str = None,\n",
    "    ):\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.device = device\n",
    "        self.save_path = Path(save_path) if save_path else save_path\n",
    "\n",
    "        self.z = next(iter(self.val_data))[0][:32].to(self.device)\n",
    "        \n",
    "        self.logs = {\n",
    "            \"Step\": [],\n",
    "            \"Train_g_loss\": [],\n",
    "            \"Train_d_loss\": [],\n",
    "            \"Val_g_loss\": [],\n",
    "            \"Val_d_loss\": [],\n",
    "            \"Samples\": [],\n",
    "        }\n",
    "\n",
    "    def init_optimizers(self, lr: float=3e-4, betas: tuple=(0.5, 0.999)):\n",
    "        self.g_optimizer = torch.optim.Adam(\n",
    "            self.generator.parameters(), lr=lr, betas=betas\n",
    "        )\n",
    "        self.d_optimizer = torch.optim.Adam(\n",
    "            self.discriminator.parameters(), lr=lr, betas=betas\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        assert (self.g_optimizer is not None) and (\n",
    "            self.d_optimizer is not None\n",
    "        ), \"Please run Trainer().init_optimizer()\"\n",
    "\n",
    "        # Loading best model\n",
    "        if self.save_path and self.save_path.exists():\n",
    "            self.load_model()\n",
    "            \n",
    "        best_score = torch.inf\n",
    "\n",
    "        for i in range(self.nb_epochs):\n",
    "            train_d_loss, train_g_loss, val_d_loss, val_g_loss = 0, 0, 0, 0\n",
    "            self.generator.train()\n",
    "            self.discriminator.train()\n",
    "            # Train loop\n",
    "            loop = tqdm(\n",
    "                enumerate(self.train_data),\n",
    "                desc=f\"Epoch {i + 1}/{self.nb_epochs} train\",\n",
    "                leave=False,\n",
    "                total=len(self.train_data),\n",
    "            )\n",
    "            for step, (x, y) in loop:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # Train step\n",
    "                g_loss, d_loss = self.train_step(x, y)\n",
    "\n",
    "                train_g_loss += g_loss\n",
    "                train_d_loss += d_loss\n",
    "\n",
    "                loop.set_postfix_str(\n",
    "                    f\"g_loss: {train_g_loss / (step + 1) :.2f}, d_loss: {train_d_loss / (step + 1) :.2f}\"\n",
    "                )\n",
    "\n",
    "            # Validation loop\n",
    "            self.generator.eval()\n",
    "            self.discriminator.eval()\n",
    "\n",
    "            loop = tqdm(\n",
    "                enumerate(self.val_data),\n",
    "                desc=f\"Epoch {i + 1}/{self.nb_epochs} validation\",\n",
    "                leave=True,\n",
    "                total=len(self.val_data),\n",
    "            )\n",
    "            for step, (x, y) in loop:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # Validation step\n",
    "                g_loss, d_loss = self.val_step(x, y)\n",
    "\n",
    "                val_g_loss += g_loss\n",
    "                val_d_loss += d_loss\n",
    "\n",
    "                loop.set_postfix_str(\n",
    "                    f\"g_loss: {val_g_loss / (step + 1) :.2f} d_loss: {val_d_loss / (step + 1) :.2f}\"\n",
    "                )\n",
    "\n",
    "            # Saving best model\n",
    "            if self.save_path and best_score > val_g_loss:\n",
    "                best_score = val_g_loss\n",
    "                self.save_model()\n",
    "\n",
    "            # Log\n",
    "            self.log_metrics(\n",
    "                step=i,\n",
    "                train_g_loss=train_g_loss,\n",
    "                train_d_loss=train_d_loss,\n",
    "                val_g_loss=val_g_loss,\n",
    "                val_d_loss=val_d_loss,\n",
    "            )\n",
    "            # plot\n",
    "            fake_img = self.generator((photos[0].unsqueeze(0)).to('cuda'))\n",
    "            fake_img = fake_img[0].cpu().detach()\n",
    "            \n",
    "            fig, ax = plt.subplots(1, 2, figsize=(20, 4))\n",
    "            ax[0].imshow(photos[0].permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "            ax[0].set_title(\"Photo\")\n",
    "            ax[0].set(xticks=[], yticks=[])\n",
    "            ax[1].imshow(fake_img.permute(1, 2, 0).numpy() * 0.5 + 0.5  )\n",
    "            ax[1].set_title(\"FakeMonet\")\n",
    "            ax[1].set(xticks=[], yticks=[])\n",
    "            plt.show()\n",
    "            \n",
    "        return self.logs\n",
    "\n",
    "    def train_step(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        y: torch.Tensor\n",
    "    ) -> tuple:\n",
    "        self.g_optimizer.zero_grad(set_to_none=True)\n",
    "        self.d_optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        fake_ = self.discriminator(\n",
    "            self.generator(x)\n",
    "        )\n",
    "        real = self.discriminator(y)\n",
    "        d_loss = (\n",
    "            torch.nn.functional.mse_loss(real, torch.ones_like(real, device=self.device)) + \n",
    "            torch.nn.functional.mse_loss(fake_, torch.zeros_like(fake_, device=self.device))\n",
    "        )\n",
    "\n",
    "        d_loss.backward()\n",
    "        self.d_optimizer.step()\n",
    "        \n",
    "        fake = self.discriminator(\n",
    "            self.generator(x)\n",
    "        )\n",
    "        \n",
    "        g_loss = torch.nn.functional.mse_loss(fake, torch.ones_like(fake, device=self.device))\n",
    "        g_loss.backward()\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        return g_loss.item(), d_loss.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def val_step(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "    ) -> tuple:\n",
    "        fake = self.discriminator(\n",
    "            self.generator(x)\n",
    "        )\n",
    "        real = self.discriminator(y)\n",
    "        g_loss = torch.nn.functional.mse_loss(fake, torch.ones_like(fake, device=self.device))\n",
    "        d_loss = (\n",
    "            torch.nn.functional.mse_loss(real, torch.ones_like(real, device=self.device)) + \n",
    "            torch.nn.functional.mse_loss(fake, torch.zeros_like(fake, device=self.device))\n",
    "        )\n",
    "        return g_loss.item(), d_loss.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def log_metrics(\n",
    "        self,\n",
    "        step: int,\n",
    "        train_g_loss: torch.Tensor,\n",
    "        train_d_loss: torch.Tensor,\n",
    "        val_g_loss: torch.Tensor,\n",
    "        val_d_loss: torch.Tensor,\n",
    "    ):\n",
    "        self.logs[\"Step\"].append(step)\n",
    "        self.logs[\"Train_g_loss\"].append(train_g_loss / len(self.train_data))\n",
    "        self.logs[\"Train_d_loss\"].append(train_d_loss / len(self.train_data))\n",
    "        self.logs[\"Val_g_loss\"].append(val_g_loss / len(self.val_data))\n",
    "        self.logs[\"Val_d_loss\"].append(val_d_loss / len(self.val_data))\n",
    "        self.logs[\"Samples\"].append(make_grid(self.generator(self.z).cpu() * 0.5 + 0.5, normalize=True))\n",
    "\n",
    "    def save_model(self, full: bool = False):\n",
    "        if full:\n",
    "            torch.save(self.generator, Path(self.save_path) / \"generator.pth\")\n",
    "            torch.save(self.discriminator, Path(self.save_path) / \"discriminator.pth\")\n",
    "        else:\n",
    "            torch.save(\n",
    "                self.generator.state_dict(),\n",
    "                Path(self.save_path) / \"generator_weights.pth\",\n",
    "            )\n",
    "            torch.save(\n",
    "                self.discriminator.state_dict(),\n",
    "                Path(self.save_path) / \"discriminator_weights.pth\",\n",
    "            )\n",
    "\n",
    "    save_dir = \"models/\"\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    def load_model(self, full: bool = False):\n",
    "        if (\n",
    "            full\n",
    "            and (self.save_path / \"generator.pth\").is_file()\n",
    "            and (self.save_path / \"discriminator.pth\").is_file()\n",
    "        ):\n",
    "            self.generator = torch.load(self.save_path / \"generator.pth\")\n",
    "            self.discriminator = torch.load(self.save_path / \"discriminator.pth\")\n",
    "        elif (\n",
    "            self.save_path / \"generator_weights.pth\"\n",
    "        ).is_file() and (\n",
    "            self.save_path / \"discriminator_weights.pth\"\n",
    "        ).is_file():\n",
    "            self.generator.load_state_dict(torch.load(self.save_path / \"generator_weights.pth\"))\n",
    "            self.discriminator.load_state_dict(torch.load(self.save_path / \"discriminator_weights.pth\"))\n",
    "\n",
    "trainer = Trainer(\n",
    "        train_data = train_dataloader,\n",
    "        val_data = test_dataloader,\n",
    "        generator = Generator(in_channels=3, features=64),\n",
    "        discriminator = Discriminator(),\n",
    "        nb_epochs = 30,\n",
    "        device = device,\n",
    "        save_path = 'models/'\n",
    "    )\n",
    "\n",
    "\n",
    "trainer.init_optimizers()\n",
    "logs = trainer.train()\n",
    "\n",
    "\n",
    "trainer.load_model()\n",
    "trainer.init_optimizers()\n",
    "logs = trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7928c8c-699f-4d30-87bf-8ad7eb63fcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python grporc",
   "language": "python",
   "name": "grproc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
